{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Parkinsons and Atypical Parkinsons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Ignore annoying warnings about future deprecations \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import h5py\n",
    "warnings.resetwarnings()\n",
    "\n",
    "### Set Random Number Seed\n",
    "np.random.seed(112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>UPDRS</th>\n",
       "      <th>aSN_FA</th>\n",
       "      <th>Caudate_FA</th>\n",
       "      <th>CC_FA</th>\n",
       "      <th>GP_FA</th>\n",
       "      <th>LobuleVI_FA</th>\n",
       "      <th>LobuleV_FA</th>\n",
       "      <th>MCP_FA</th>\n",
       "      <th>...</th>\n",
       "      <th>LobuleVI_FW</th>\n",
       "      <th>LobuleV_FW</th>\n",
       "      <th>MCP_FW</th>\n",
       "      <th>pSN_FW</th>\n",
       "      <th>Putamen_FW</th>\n",
       "      <th>SCP_FW</th>\n",
       "      <th>STN_FW</th>\n",
       "      <th>Thalamus_FW</th>\n",
       "      <th>Vermis_FW</th>\n",
       "      <th>GroupID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.645260</td>\n",
       "      <td>0.237540</td>\n",
       "      <td>0.658996</td>\n",
       "      <td>0.390937</td>\n",
       "      <td>0.199762</td>\n",
       "      <td>0.248622</td>\n",
       "      <td>0.583646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188899</td>\n",
       "      <td>0.253751</td>\n",
       "      <td>0.098390</td>\n",
       "      <td>0.173284</td>\n",
       "      <td>0.159989</td>\n",
       "      <td>0.415654</td>\n",
       "      <td>0.124107</td>\n",
       "      <td>0.160673</td>\n",
       "      <td>0.204663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.654966</td>\n",
       "      <td>0.222827</td>\n",
       "      <td>0.607287</td>\n",
       "      <td>0.383109</td>\n",
       "      <td>0.225891</td>\n",
       "      <td>0.275686</td>\n",
       "      <td>0.635246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133954</td>\n",
       "      <td>0.140164</td>\n",
       "      <td>0.086658</td>\n",
       "      <td>0.195432</td>\n",
       "      <td>0.169670</td>\n",
       "      <td>0.308992</td>\n",
       "      <td>0.142579</td>\n",
       "      <td>0.149033</td>\n",
       "      <td>0.146168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.674251</td>\n",
       "      <td>0.236799</td>\n",
       "      <td>0.743437</td>\n",
       "      <td>0.324580</td>\n",
       "      <td>0.224945</td>\n",
       "      <td>0.237046</td>\n",
       "      <td>0.636315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228307</td>\n",
       "      <td>0.299650</td>\n",
       "      <td>0.078886</td>\n",
       "      <td>0.137439</td>\n",
       "      <td>0.185490</td>\n",
       "      <td>0.354840</td>\n",
       "      <td>0.096919</td>\n",
       "      <td>0.140484</td>\n",
       "      <td>0.202398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.664740</td>\n",
       "      <td>0.221283</td>\n",
       "      <td>0.702845</td>\n",
       "      <td>0.346713</td>\n",
       "      <td>0.224896</td>\n",
       "      <td>0.258485</td>\n",
       "      <td>0.632627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>0.161162</td>\n",
       "      <td>0.081963</td>\n",
       "      <td>0.172358</td>\n",
       "      <td>0.122145</td>\n",
       "      <td>0.380896</td>\n",
       "      <td>0.085467</td>\n",
       "      <td>0.080924</td>\n",
       "      <td>0.167342</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.656800</td>\n",
       "      <td>0.204532</td>\n",
       "      <td>0.692499</td>\n",
       "      <td>0.322482</td>\n",
       "      <td>0.250622</td>\n",
       "      <td>0.225412</td>\n",
       "      <td>0.575726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131577</td>\n",
       "      <td>0.170267</td>\n",
       "      <td>0.085887</td>\n",
       "      <td>0.244718</td>\n",
       "      <td>0.141260</td>\n",
       "      <td>0.427207</td>\n",
       "      <td>0.117136</td>\n",
       "      <td>0.156742</td>\n",
       "      <td>0.169126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Sex  UPDRS    aSN_FA  Caudate_FA     CC_FA     GP_FA  LobuleVI_FA  \\\n",
       "0   68    1    0.0  0.645260    0.237540  0.658996  0.390937     0.199762   \n",
       "1   53    1    0.0  0.654966    0.222827  0.607287  0.383109     0.225891   \n",
       "2   67    0    1.0  0.674251    0.236799  0.743437  0.324580     0.224945   \n",
       "3   46    1    4.7  0.664740    0.221283  0.702845  0.346713     0.224896   \n",
       "4   45    0    2.0  0.656800    0.204532  0.692499  0.322482     0.250622   \n",
       "\n",
       "   LobuleV_FA    MCP_FA   ...     LobuleVI_FW  LobuleV_FW    MCP_FW    pSN_FW  \\\n",
       "0    0.248622  0.583646   ...        0.188899    0.253751  0.098390  0.173284   \n",
       "1    0.275686  0.635246   ...        0.133954    0.140164  0.086658  0.195432   \n",
       "2    0.237046  0.636315   ...        0.228307    0.299650  0.078886  0.137439   \n",
       "3    0.258485  0.632627   ...        0.195933    0.161162  0.081963  0.172358   \n",
       "4    0.225412  0.575726   ...        0.131577    0.170267  0.085887  0.244718   \n",
       "\n",
       "   Putamen_FW    SCP_FW    STN_FW  Thalamus_FW  Vermis_FW  GroupID  \n",
       "0    0.159989  0.415654  0.124107     0.160673   0.204663        0  \n",
       "1    0.169670  0.308992  0.142579     0.149033   0.146168        0  \n",
       "2    0.185490  0.354840  0.096919     0.140484   0.202398        0  \n",
       "3    0.122145  0.380896  0.085467     0.080924   0.167342        0  \n",
       "4    0.141260  0.427207  0.117136     0.156742   0.169126        0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"Budathon_2018.xlsx\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controls vs Any Parkinsonism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group the Parkinsons subjects into a single common group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "df.loc[df['GroupID'] != 0, 'GroupID'] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [col for col in df.columns if col != 'GroupID']\n",
    "X = pd.DataFrame(df, columns= x_cols)\n",
    "Y = df['GroupID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and compile the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtbricker\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    }
   ],
   "source": [
    "input_len = len(X.columns)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_len, input_dim=input_len, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',auc_roc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deine some custom functions to measure AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtbricker\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "746/746 [==============================] - 1s 1ms/step - loss: 0.6841 - acc: 0.8043 - auc: 0.7304\n",
      "Epoch 2/150\n",
      "746/746 [==============================] - 0s 302us/step - loss: 0.1951 - acc: 0.9383 - auc: 0.8844\n",
      "Epoch 3/150\n",
      "746/746 [==============================] - 0s 306us/step - loss: 0.1768 - acc: 0.9504 - auc: 0.9221\n",
      "Epoch 4/150\n",
      "746/746 [==============================] - 0s 272us/step - loss: 0.1699 - acc: 0.9544 - auc: 0.9375\n",
      "Epoch 5/150\n",
      "746/746 [==============================] - 0s 258us/step - loss: 0.1726 - acc: 0.9450 - auc: 0.9481\n",
      "Epoch 6/150\n",
      "746/746 [==============================] - 0s 298us/step - loss: 0.1653 - acc: 0.9531 - auc: 0.9529\n",
      "Epoch 7/150\n",
      "746/746 [==============================] - 0s 292us/step - loss: 0.1645 - acc: 0.9477 - auc: 0.9572\n",
      "Epoch 8/150\n",
      "746/746 [==============================] - 0s 265us/step - loss: 0.1597 - acc: 0.9464 - auc: 0.9605\n",
      "Epoch 9/150\n",
      "746/746 [==============================] - 0s 268us/step - loss: 0.1666 - acc: 0.9491 - auc: 0.9626\n",
      "Epoch 10/150\n",
      "746/746 [==============================] - 0s 294us/step - loss: 0.1569 - acc: 0.9558 - auc: 0.9642\n",
      "Epoch 11/150\n",
      "746/746 [==============================] - 0s 274us/step - loss: 0.1654 - acc: 0.9531 - auc: 0.9658\n",
      "Epoch 12/150\n",
      "746/746 [==============================] - 0s 271us/step - loss: 0.1651 - acc: 0.9477 - auc: 0.9668\n",
      "Epoch 13/150\n",
      "746/746 [==============================] - 0s 286us/step - loss: 0.1542 - acc: 0.9571 - auc: 0.9678\n",
      "Epoch 14/150\n",
      "746/746 [==============================] - 0s 308us/step - loss: 0.1600 - acc: 0.9464 - auc: 0.9686\n",
      "Epoch 15/150\n",
      "746/746 [==============================] - 0s 302us/step - loss: 0.1580 - acc: 0.9544 - auc: 0.9694\n",
      "Epoch 16/150\n",
      "746/746 [==============================] - 0s 278us/step - loss: 0.1522 - acc: 0.9491 - auc: 0.9702\n",
      "Epoch 17/150\n",
      "746/746 [==============================] - 0s 273us/step - loss: 0.1563 - acc: 0.9410 - auc: 0.9705\n",
      "Epoch 18/150\n",
      "746/746 [==============================] - 0s 305us/step - loss: 0.1546 - acc: 0.9504 - auc: 0.9710\n",
      "Epoch 19/150\n",
      "746/746 [==============================] - 0s 352us/step - loss: 0.1648 - acc: 0.9477 - auc: 0.9714\n",
      "Epoch 20/150\n",
      "746/746 [==============================] - 0s 383us/step - loss: 0.1568 - acc: 0.9464 - auc: 0.9718\n",
      "Epoch 21/150\n",
      "746/746 [==============================] - 0s 394us/step - loss: 0.1433 - acc: 0.9477 - auc: 0.9723\n",
      "Epoch 22/150\n",
      "746/746 [==============================] - 0s 305us/step - loss: 0.1488 - acc: 0.9504 - auc: 0.9726\n",
      "Epoch 23/150\n",
      "746/746 [==============================] - 0s 313us/step - loss: 0.1497 - acc: 0.9450 - auc: 0.9730\n",
      "Epoch 24/150\n",
      "746/746 [==============================] - 0s 420us/step - loss: 0.1512 - acc: 0.9517 - auc: 0.9732\n",
      "Epoch 25/150\n",
      "746/746 [==============================] - 0s 315us/step - loss: 0.1454 - acc: 0.9517 - auc: 0.9734\n",
      "Epoch 26/150\n",
      "746/746 [==============================] - 0s 300us/step - loss: 0.1441 - acc: 0.9584 - auc: 0.9736\n",
      "Epoch 27/150\n",
      "746/746 [==============================] - 0s 328us/step - loss: 0.1512 - acc: 0.9531 - auc: 0.9739\n",
      "Epoch 28/150\n",
      "746/746 [==============================] - 0s 348us/step - loss: 0.1454 - acc: 0.9531 - auc: 0.9740\n",
      "Epoch 29/150\n",
      "746/746 [==============================] - 0s 297us/step - loss: 0.1416 - acc: 0.9571 - auc: 0.9743\n",
      "Epoch 30/150\n",
      "746/746 [==============================] - 0s 331us/step - loss: 0.1483 - acc: 0.9491 - auc: 0.9744\n",
      "Epoch 31/150\n",
      "746/746 [==============================] - 0s 328us/step - loss: 0.1514 - acc: 0.9517 - auc: 0.9746\n",
      "Epoch 32/150\n",
      "746/746 [==============================] - 0s 273us/step - loss: 0.1384 - acc: 0.9584 - auc: 0.9748\n",
      "Epoch 33/150\n",
      "746/746 [==============================] - 0s 271us/step - loss: 0.1560 - acc: 0.9491 - auc: 0.9750\n",
      "Epoch 34/150\n",
      "746/746 [==============================] - 0s 275us/step - loss: 0.1476 - acc: 0.9450 - auc: 0.9751\n",
      "Epoch 35/150\n",
      "746/746 [==============================] - 0s 257us/step - loss: 0.1472 - acc: 0.9544 - auc: 0.9753\n",
      "Epoch 36/150\n",
      "746/746 [==============================] - 0s 339us/step - loss: 0.1435 - acc: 0.9517 - auc: 0.9754\n",
      "Epoch 37/150\n",
      "746/746 [==============================] - 0s 272us/step - loss: 0.1429 - acc: 0.9504 - auc: 0.9756\n",
      "Epoch 38/150\n",
      "746/746 [==============================] - 0s 267us/step - loss: 0.1437 - acc: 0.9531 - auc: 0.9756\n",
      "Epoch 39/150\n",
      "746/746 [==============================] - 0s 264us/step - loss: 0.1463 - acc: 0.9504 - auc: 0.9757\n",
      "Epoch 40/150\n",
      "746/746 [==============================] - 0s 265us/step - loss: 0.1424 - acc: 0.9584 - auc: 0.9760\n",
      "Epoch 41/150\n",
      "746/746 [==============================] - 0s 286us/step - loss: 0.1449 - acc: 0.9517 - auc: 0.9760\n",
      "Epoch 42/150\n",
      "746/746 [==============================] - 0s 270us/step - loss: 0.1541 - acc: 0.9450 - auc: 0.9761\n",
      "Epoch 43/150\n",
      "746/746 [==============================] - 0s 269us/step - loss: 0.1388 - acc: 0.9504 - auc: 0.9763\n",
      "Epoch 44/150\n",
      "746/746 [==============================] - 0s 312us/step - loss: 0.1396 - acc: 0.9584 - auc: 0.9764\n",
      "Epoch 45/150\n",
      "746/746 [==============================] - 0s 275us/step - loss: 0.1422 - acc: 0.9544 - auc: 0.9764\n",
      "Epoch 46/150\n",
      "746/746 [==============================] - 0s 322us/step - loss: 0.1416 - acc: 0.9477 - auc: 0.9766\n",
      "Epoch 47/150\n",
      "746/746 [==============================] - 0s 343us/step - loss: 0.1427 - acc: 0.9571 - auc: 0.9766\n",
      "Epoch 48/150\n",
      "746/746 [==============================] - 0s 345us/step - loss: 0.1451 - acc: 0.9531 - auc: 0.9767\n",
      "Epoch 49/150\n",
      "746/746 [==============================] - 0s 245us/step - loss: 0.1504 - acc: 0.9491 - auc: 0.9768\n",
      "Epoch 50/150\n",
      "746/746 [==============================] - 0s 307us/step - loss: 0.1466 - acc: 0.9504 - auc: 0.9769\n",
      "Epoch 51/150\n",
      "746/746 [==============================] - 0s 347us/step - loss: 0.1412 - acc: 0.9504 - auc: 0.9769\n",
      "Epoch 52/150\n",
      "746/746 [==============================] - 0s 283us/step - loss: 0.1396 - acc: 0.9450 - auc: 0.9771\n",
      "Epoch 53/150\n",
      "746/746 [==============================] - 0s 252us/step - loss: 0.1370 - acc: 0.9504 - auc: 0.9771\n",
      "Epoch 54/150\n",
      "746/746 [==============================] - 0s 280us/step - loss: 0.1426 - acc: 0.9504 - auc: 0.9773\n",
      "Epoch 55/150\n",
      "746/746 [==============================] - 0s 277us/step - loss: 0.1472 - acc: 0.9531 - auc: 0.9774\n",
      "Epoch 56/150\n",
      "746/746 [==============================] - 0s 298us/step - loss: 0.1387 - acc: 0.9491 - auc: 0.9774\n",
      "Epoch 57/150\n",
      "746/746 [==============================] - 0s 288us/step - loss: 0.1408 - acc: 0.9544 - auc: 0.9775\n",
      "Epoch 58/150\n",
      "746/746 [==============================] - 0s 373us/step - loss: 0.1437 - acc: 0.9491 - auc: 0.9776\n",
      "Epoch 59/150\n",
      "746/746 [==============================] - 0s 314us/step - loss: 0.1387 - acc: 0.9504 - auc: 0.9776\n",
      "Epoch 60/150\n",
      "746/746 [==============================] - 0s 303us/step - loss: 0.1435 - acc: 0.9477 - auc: 0.9778\n",
      "Epoch 61/150\n",
      "746/746 [==============================] - 0s 262us/step - loss: 0.1325 - acc: 0.9584 - auc: 0.9779\n",
      "Epoch 62/150\n",
      "746/746 [==============================] - 0s 272us/step - loss: 0.1401 - acc: 0.9517 - auc: 0.9780\n",
      "Epoch 63/150\n",
      "746/746 [==============================] - 0s 290us/step - loss: 0.1430 - acc: 0.9598 - auc: 0.9780\n",
      "Epoch 64/150\n",
      "746/746 [==============================] - 0s 332us/step - loss: 0.1368 - acc: 0.9544 - auc: 0.9781\n",
      "Epoch 65/150\n",
      "746/746 [==============================] - 0s 323us/step - loss: 0.1465 - acc: 0.9491 - auc: 0.9781\n",
      "Epoch 66/150\n",
      "746/746 [==============================] - 0s 252us/step - loss: 0.1406 - acc: 0.9491 - auc: 0.9782\n",
      "Epoch 67/150\n",
      "746/746 [==============================] - 0s 278us/step - loss: 0.1412 - acc: 0.9531 - auc: 0.9783\n",
      "Epoch 68/150\n",
      "746/746 [==============================] - 0s 272us/step - loss: 0.1392 - acc: 0.9544 - auc: 0.9783\n",
      "Epoch 69/150\n",
      "746/746 [==============================] - 0s 324us/step - loss: 0.1495 - acc: 0.9491 - auc: 0.9783\n",
      "Epoch 70/150\n",
      "746/746 [==============================] - 0s 317us/step - loss: 0.1380 - acc: 0.9531 - auc: 0.9784\n",
      "Epoch 71/150\n",
      "746/746 [==============================] - 0s 266us/step - loss: 0.1265 - acc: 0.9598 - auc: 0.9784\n",
      "Epoch 72/150\n",
      "746/746 [==============================] - 0s 287us/step - loss: 0.1348 - acc: 0.9571 - auc: 0.9786\n",
      "Epoch 73/150\n",
      "746/746 [==============================] - 0s 319us/step - loss: 0.1401 - acc: 0.9504 - auc: 0.9787\n",
      "Epoch 74/150\n",
      "746/746 [==============================] - 0s 329us/step - loss: 0.1427 - acc: 0.9517 - auc: 0.9787\n",
      "Epoch 75/150\n",
      "746/746 [==============================] - 0s 296us/step - loss: 0.1372 - acc: 0.9531 - auc: 0.9787\n",
      "Epoch 76/150\n",
      "746/746 [==============================] - 0s 272us/step - loss: 0.1421 - acc: 0.9531 - auc: 0.9788\n",
      "Epoch 77/150\n",
      "746/746 [==============================] - 0s 268us/step - loss: 0.1310 - acc: 0.9571 - auc: 0.9789\n",
      "Epoch 78/150\n",
      "746/746 [==============================] - 0s 292us/step - loss: 0.1343 - acc: 0.9531 - auc: 0.9789\n",
      "Epoch 79/150\n",
      "746/746 [==============================] - 0s 315us/step - loss: 0.1302 - acc: 0.9531 - auc: 0.9790\n",
      "Epoch 80/150\n",
      "746/746 [==============================] - 0s 343us/step - loss: 0.1397 - acc: 0.9517 - auc: 0.9791\n",
      "Epoch 81/150\n",
      "746/746 [==============================] - 0s 356us/step - loss: 0.1324 - acc: 0.9531 - auc: 0.9791\n",
      "Epoch 82/150\n",
      "746/746 [==============================] - 0s 317us/step - loss: 0.1388 - acc: 0.9531 - auc: 0.9791\n",
      "Epoch 83/150\n",
      "746/746 [==============================] - 0s 287us/step - loss: 0.1410 - acc: 0.9571 - auc: 0.9792\n",
      "Epoch 84/150\n",
      "746/746 [==============================] - 0s 327us/step - loss: 0.1373 - acc: 0.9611 - auc: 0.9792\n",
      "Epoch 85/150\n",
      "746/746 [==============================] - 0s 263us/step - loss: 0.1384 - acc: 0.9531 - auc: 0.9792\n",
      "Epoch 86/150\n",
      "746/746 [==============================] - 0s 288us/step - loss: 0.1334 - acc: 0.9584 - auc: 0.9793\n",
      "Epoch 87/150\n",
      "746/746 [==============================] - 0s 261us/step - loss: 0.1295 - acc: 0.9544 - auc: 0.9793\n",
      "Epoch 88/150\n",
      "746/746 [==============================] - 0s 284us/step - loss: 0.1332 - acc: 0.9531 - auc: 0.9794\n",
      "Epoch 89/150\n",
      "746/746 [==============================] - 0s 286us/step - loss: 0.1281 - acc: 0.9598 - auc: 0.9794\n",
      "Epoch 90/150\n",
      "746/746 [==============================] - 0s 316us/step - loss: 0.1315 - acc: 0.9558 - auc: 0.9795\n",
      "Epoch 91/150\n",
      "746/746 [==============================] - 0s 288us/step - loss: 0.1335 - acc: 0.9544 - auc: 0.9796\n",
      "Epoch 92/150\n",
      "746/746 [==============================] - 0s 255us/step - loss: 0.1399 - acc: 0.9531 - auc: 0.9797\n",
      "Epoch 93/150\n",
      "746/746 [==============================] - 0s 277us/step - loss: 0.1303 - acc: 0.9611 - auc: 0.9797\n",
      "Epoch 94/150\n",
      "746/746 [==============================] - 0s 300us/step - loss: 0.1308 - acc: 0.9491 - auc: 0.9797\n",
      "Epoch 95/150\n",
      "746/746 [==============================] - 0s 262us/step - loss: 0.1278 - acc: 0.9584 - auc: 0.9798\n",
      "Epoch 96/150\n",
      "746/746 [==============================] - 0s 273us/step - loss: 0.1255 - acc: 0.9531 - auc: 0.9799\n",
      "Epoch 97/150\n",
      "746/746 [==============================] - 0s 302us/step - loss: 0.1297 - acc: 0.9598 - auc: 0.9800\n",
      "Epoch 98/150\n",
      "746/746 [==============================] - 0s 331us/step - loss: 0.1390 - acc: 0.9491 - auc: 0.9800\n",
      "Epoch 99/150\n",
      "746/746 [==============================] - 0s 270us/step - loss: 0.1286 - acc: 0.9598 - auc: 0.9800\n",
      "Epoch 100/150\n",
      "746/746 [==============================] - 0s 269us/step - loss: 0.1294 - acc: 0.9584 - auc: 0.9801\n",
      "Epoch 101/150\n",
      "746/746 [==============================] - 0s 248us/step - loss: 0.1246 - acc: 0.9598 - auc: 0.9801\n",
      "Epoch 102/150\n",
      "746/746 [==============================] - 0s 295us/step - loss: 0.1275 - acc: 0.9571 - auc: 0.9802\n",
      "Epoch 103/150\n",
      "746/746 [==============================] - 0s 282us/step - loss: 0.1256 - acc: 0.9544 - auc: 0.9803\n",
      "Epoch 104/150\n",
      "746/746 [==============================] - 0s 291us/step - loss: 0.1280 - acc: 0.9544 - auc: 0.9803\n",
      "Epoch 105/150\n",
      "746/746 [==============================] - 0s 267us/step - loss: 0.1274 - acc: 0.9584 - auc: 0.9804\n",
      "Epoch 106/150\n",
      "746/746 [==============================] - 0s 306us/step - loss: 0.1283 - acc: 0.9598 - auc: 0.9804\n",
      "Epoch 107/150\n",
      "746/746 [==============================] - 0s 265us/step - loss: 0.1287 - acc: 0.9611 - auc: 0.9804\n",
      "Epoch 108/150\n",
      "746/746 [==============================] - 0s 245us/step - loss: 0.1213 - acc: 0.9625 - auc: 0.9805\n",
      "Epoch 109/150\n",
      "746/746 [==============================] - 0s 251us/step - loss: 0.1234 - acc: 0.9571 - auc: 0.9806\n",
      "Epoch 110/150\n",
      "746/746 [==============================] - 0s 250us/step - loss: 0.1391 - acc: 0.9611 - auc: 0.9806\n",
      "Epoch 111/150\n",
      "746/746 [==============================] - 0s 252us/step - loss: 0.1300 - acc: 0.9544 - auc: 0.9806\n",
      "Epoch 112/150\n",
      "746/746 [==============================] - 0s 277us/step - loss: 0.1278 - acc: 0.9571 - auc: 0.9807\n",
      "Epoch 113/150\n",
      "746/746 [==============================] - 0s 260us/step - loss: 0.1252 - acc: 0.9584 - auc: 0.9807\n",
      "Epoch 114/150\n",
      "746/746 [==============================] - 0s 270us/step - loss: 0.1531 - acc: 0.9504 - auc: 0.9807\n",
      "Epoch 115/150\n",
      "746/746 [==============================] - 0s 293us/step - loss: 0.1303 - acc: 0.9584 - auc: 0.9808\n",
      "Epoch 116/150\n",
      "746/746 [==============================] - 0s 267us/step - loss: 0.1231 - acc: 0.9571 - auc: 0.9808\n",
      "Epoch 117/150\n",
      "746/746 [==============================] - 0s 248us/step - loss: 0.1230 - acc: 0.9544 - auc: 0.9809\n",
      "Epoch 118/150\n",
      "746/746 [==============================] - 0s 266us/step - loss: 0.1310 - acc: 0.9544 - auc: 0.9809\n",
      "Epoch 119/150\n",
      "746/746 [==============================] - 0s 267us/step - loss: 0.1224 - acc: 0.9584 - auc: 0.9810\n",
      "Epoch 120/150\n",
      "746/746 [==============================] - 0s 284us/step - loss: 0.1274 - acc: 0.9558 - auc: 0.9810\n",
      "Epoch 121/150\n",
      "746/746 [==============================] - 0s 257us/step - loss: 0.1283 - acc: 0.9571 - auc: 0.9811\n",
      "Epoch 122/150\n",
      "746/746 [==============================] - 0s 248us/step - loss: 0.1235 - acc: 0.9544 - auc: 0.9811\n",
      "Epoch 123/150\n",
      "746/746 [==============================] - 0s 249us/step - loss: 0.1193 - acc: 0.9584 - auc: 0.9812\n",
      "Epoch 124/150\n",
      "746/746 [==============================] - 0s 250us/step - loss: 0.1242 - acc: 0.9584 - auc: 0.9812\n",
      "Epoch 125/150\n",
      "746/746 [==============================] - 0s 246us/step - loss: 0.1278 - acc: 0.9517 - auc: 0.9813\n",
      "Epoch 126/150\n",
      "746/746 [==============================] - 0s 246us/step - loss: 0.1188 - acc: 0.9558 - auc: 0.9813\n",
      "Epoch 127/150\n",
      "746/746 [==============================] - 0s 254us/step - loss: 0.1219 - acc: 0.9517 - auc: 0.9814\n",
      "Epoch 128/150\n",
      "746/746 [==============================] - 0s 262us/step - loss: 0.1272 - acc: 0.9544 - auc: 0.9814\n",
      "Epoch 129/150\n",
      "746/746 [==============================] - 0s 310us/step - loss: 0.1228 - acc: 0.9598 - auc: 0.9815\n",
      "Epoch 130/150\n",
      "746/746 [==============================] - 0s 256us/step - loss: 0.1221 - acc: 0.9558 - auc: 0.9815\n",
      "Epoch 131/150\n",
      "746/746 [==============================] - 0s 245us/step - loss: 0.1173 - acc: 0.9638 - auc: 0.9816\n",
      "Epoch 132/150\n",
      "746/746 [==============================] - 0s 238us/step - loss: 0.1238 - acc: 0.9571 - auc: 0.9816\n",
      "Epoch 133/150\n",
      "746/746 [==============================] - 0s 262us/step - loss: 0.1273 - acc: 0.9531 - auc: 0.9817\n",
      "Epoch 134/150\n",
      "746/746 [==============================] - 0s 279us/step - loss: 0.1227 - acc: 0.9598 - auc: 0.9817\n",
      "Epoch 135/150\n",
      "746/746 [==============================] - 0s 246us/step - loss: 0.1173 - acc: 0.9598 - auc: 0.9818\n",
      "Epoch 136/150\n",
      "746/746 [==============================] - 0s 290us/step - loss: 0.1265 - acc: 0.9531 - auc: 0.9818\n",
      "Epoch 137/150\n",
      "746/746 [==============================] - 0s 246us/step - loss: 0.1149 - acc: 0.9638 - auc: 0.9819\n",
      "Epoch 138/150\n",
      "746/746 [==============================] - 0s 262us/step - loss: 0.1271 - acc: 0.9571 - auc: 0.9819\n",
      "Epoch 139/150\n",
      "746/746 [==============================] - 0s 269us/step - loss: 0.1305 - acc: 0.9598 - auc: 0.9820\n",
      "Epoch 140/150\n",
      "746/746 [==============================] - 0s 263us/step - loss: 0.1276 - acc: 0.9571 - auc: 0.9820\n",
      "Epoch 141/150\n",
      "746/746 [==============================] - 0s 250us/step - loss: 0.1203 - acc: 0.9584 - auc: 0.9820\n",
      "Epoch 142/150\n",
      "746/746 [==============================] - 0s 286us/step - loss: 0.1218 - acc: 0.9638 - auc: 0.9821\n",
      "Epoch 143/150\n",
      "746/746 [==============================] - 0s 247us/step - loss: 0.1245 - acc: 0.9584 - auc: 0.9821\n",
      "Epoch 144/150\n",
      "746/746 [==============================] - 0s 242us/step - loss: 0.1167 - acc: 0.9598 - auc: 0.9821\n",
      "Epoch 145/150\n",
      "746/746 [==============================] - 0s 250us/step - loss: 0.1219 - acc: 0.9504 - auc: 0.9822\n",
      "Epoch 146/150\n",
      "746/746 [==============================] - 0s 250us/step - loss: 0.1202 - acc: 0.9638 - auc: 0.9822\n",
      "Epoch 147/150\n",
      "746/746 [==============================] - 0s 255us/step - loss: 0.1206 - acc: 0.9598 - auc: 0.9823\n",
      "Epoch 148/150\n",
      "746/746 [==============================] - 0s 254us/step - loss: 0.1184 - acc: 0.9571 - auc: 0.9823\n",
      "Epoch 149/150\n",
      "746/746 [==============================] - 0s 243us/step - loss: 0.1190 - acc: 0.9598 - auc: 0.9824\n",
      "Epoch 150/150\n",
      "746/746 [==============================] - 0s 235us/step - loss: 0.1157 - acc: 0.9598 - auc: 0.9824\n",
      "746/746 [==============================] - 0s 273us/step\n",
      "\n",
      "acc: 96.11%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=150, batch_size=10)\n",
    "\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
