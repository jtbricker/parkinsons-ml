{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parkinson's Machine Learning Classification\n",
    "*Goal: To create a machine learning algorithm which will distinguish Parkinsonâ€™s Disease, Multiple System Atrophy, and Progressive Supranuclear Palsy using an automated pipeline.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Initial Data Analysis](#Initial-Data-Analysis)\n",
    "1. [Initial Model Benchmarks](#Initial-Model-Benchmarks)\n",
    "    1. [Benchmark Results](#Results)\n",
    "1. [Preprocessing](#Preprocessing)\n",
    "    1. [Standardization](#Standardization)\n",
    "    1. [Feature Subset Selection](#Feature-Subset-Selection)\n",
    "    1. [Dimensionality Reduction](#Dimensionality-Reduction)\n",
    "        1. [Principal Component Analysis](#Principal-Component-Analysis)\n",
    "        1. [Linear Discriminant Analysis](#Linear-Discriminant-Analysis)\n",
    "1. [Model Optimization **(new!)**](#Model-Optimization)\n",
    "    1. [Random Forest](#Random-Forest)\n",
    "1. [Notes](#Notes)\n",
    "    1. [6/28/2018- Resampling before cross-validation](#6/28/2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Analysis \n",
    "[See Data Exploration Notebook](Data_Exploration.ipynb)\n",
    "\n",
    "The data set obviously has a class imbalance (Control 32%, Parkinsons 53%, MSA 7%, PSP 7%).  We will need to resample the data or use an algorithm like Decision Trees that are not influenced as heavily by class imbalance.  We should also take care to see how well the under-represented classes are being predicted when evaluating accuracy of our models.\n",
    "\n",
    "Feature appear relatively normally-distributed between groups.  Nothing particularly interesting jumping out by looking at covariance and pearson correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Benchmarks\n",
    "[See Modeling Benchmark Notebook](ModelingBenchmarks.ipynb)\n",
    "\n",
    "8 initial models were tested for __multiclass classification__ (I would ideally like to avoid composing results of multiple binary classifiers since the cost of that composition is great):\n",
    "* Logistic Regression (*log*)\n",
    "* SVC with Linear Kernel (*svc_lin*)\n",
    "* SVC with RBF Kernel (*svc_rbf*)\n",
    "* k-Nearest Neighbors (*knn*)\n",
    "* Decision Trees (*rand_for*)\n",
    "* Artificial Neural Networks (*ann*)\n",
    "* Naive Bayes (*gnb*)\n",
    "* AdaBoost (*ada*)\n",
    "\n",
    "These models were run with 5-fold (stratified) cross validation on 80% of the training data (20% kept for holdout validation).  To address the problem of class imbalance, the data was resampled (upsampling of the minority classes) to ensure that the minority classes were well represented in the models.  Alternative imbalanced class corrections can be explored in the future.   No additional preprocessing of the data was performed (scaling, feature selection, etc..).\n",
    "\n",
    "For most models, default hyperparameters were used (with minor tweaking) based on experience.\n",
    "\n",
    "The \"Outside Validation\" data set (who sites were not included in the training data at all), was tested to measure generalizability to unseen data.\n",
    "\n",
    "The models were evaluated by several means with particular attention paid to the performance on the minority classes:\n",
    "* Cross Validation Score (Mean Accuracy of the folds)\n",
    "* Holdout Data accuracy, precision (PPV), and recall (sensitivity)\n",
    "* \"Outside Validation\" Data accuracy, precision (PPV), and recall (sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Initial Model Accuracies'](images/benchmark_accuracies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k nearest neighbors, non-linear support vector, random forests, and logorithmic regression yielded cross-validation and holdout accuracies > 80%.\n",
    "\n",
    "Each of those classifiers yield \"Outside Validation\" accuracies significantly less than 80%.  However, the other classifiers behave consistently between cross-validation, holdout, and validation data sets.\n",
    "\n",
    "Of course, significant improvements are expected when we begin to introduce various preprocessing methods.\n",
    "\n",
    "Further useful insight into the how well these models are performing on the individual class level can be seen by viewing the confusion matrices in \n",
    "[the Modeling Benchmark Notebook](Model Benchmarks/ModelingBenchmarks.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Standardization\n",
    "[See Model Search- Standardization Notebook](ModelSearch-Standardization.ipynb)\n",
    "\n",
    "We will use the standard scalar to force each feature column to have a mean of zero and a variance of 1.  This is a requirement of many algorithms.\n",
    "\n",
    "Most of the features are already [0,1] bound with low variance.  The major exceptions are Age and UPDRS.\n",
    "\n",
    "!['Accuracies with Standardization'](images/standardization_acc.png)\n",
    "\n",
    "With the standardization step, most models' accuracies do no change much.  However the linear svc and neural network models see dramatic improvements.\n",
    "\n",
    "!['Cross Validations Accuracies with Standardization'](images/standardization_validation.png)\n",
    "\n",
    "In this image, the cross-validation accuracy is the mean of the accuracies of all 5 models trained during 5-fold cross validation. The holdout and validation accuracies are the accuracies obtained by training the models on the **entire** training set and passing the standardized holdout and outside validation data to the trained model.\n",
    "\n",
    "Looking at the accuracies of these models with the holdout and outside validation data, we see some regression.  This suggests overfitting (it also looks like our *svc_rbf* model is also always predicting a single class).  We will address overfitting issues when do our hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Subset Selection\n",
    "[See ModelSearch- k Best Features Notebook](ModelSearch-kBestFeatures.ipynb)\n",
    "\n",
    "We will do further data preprocessing, this time training models on the **k-best** features.  \"Best\" here is determined by which features have the largest dependence on the class target variable.  We will use ANOVA F-score as our measure of dependence (Mutual Information was also compared, but showed no significant improvement was much slower).  This preprocessing will be done **in addition to the the standardization step from the previous section**.\n",
    "\n",
    "!['K-Best Feature Selection'](images/kBest_fscore.png)\n",
    "\n",
    "In the above figure, we see several models where the maximum (cross-validation) accuracy is at a k < 37, suggesting that we can get a performance improvement by feature subset selection.\n",
    "\n",
    "For each model, we will find the value of **k** which maximizes the mean cross-validation accuracy.\n",
    "\n",
    "|    _model_    | knn | svc_lin | svc_rbf | rand_for | ada | gnb | log | ann |\n",
    "|---------------|-----|---------|---------|----------|-----|-----|-----|-----|\n",
    "| ** _best_ k** |  4  | 33      |    3    |  20      |  26 |  9  |  20 |  32 |\n",
    "\n",
    "\n",
    "Just as in the standardization preprocessing section, we will compare the **k-best** cross-validation accuracy with that of the initial model benchmarks and we will see a comparison of the cross-validation, holdout, and outside validation accuracies using this method.\n",
    "\n",
    "!['Feature Selection Comparison w/ Standardization'](images/feature_subset_acc_fscore.png)\n",
    "!['Feature Selection Validation'](images/fss_validation_fscore.png)\n",
    "\n",
    "We see mild performance improvement in most of the models.  We should therefor include feature selection in our model pipeline and tune **k** during the optimization stage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "[See ModelSearch- Dimensionality Reduction notebook](ModelSearch-DimensionalityReduction.ipynb)\n",
    "We are going to reduce the number of dimensions of the data using Principle Component Analysis (PCA) and Linear Discriminant Analysis (LDA).  Both methods reduce the data to fewer dimensions by projecting in the directions of greatest variance. This can reduce noise and improve performance. LDA also takes the class into account in order to maximize differences between classes in the projected data.\n",
    "\n",
    "We will include the standardization step from the previous stage.\n",
    "\n",
    "#### Principal Component Analysis\n",
    "PCA did not yield particularly useful results.  As can be seen in the graphs below, we get significantly accuracy for most models by using all available components and therefore its not beneficial to project out data via PCA unless we need to improve computational speed.\n",
    "\n",
    "!['PCA Accuracy vs Number of Components'](images/pca_ns.png)\n",
    "\n",
    "#### Linear Discriminant Analysis\n",
    "LDA always projects the data to a number of dimensions up to the number of different classes - 1.  It generally best to use all components.\n",
    "In the graph below we compare accuracies of our pipeline with only standardization and with standardization and the LDA projection.  We also observe how the transformation affects accuracy accross our training, holdout, and outside validation data.\n",
    "\n",
    "!['PCA Accuracy vs Number of Components'](images/lda_acc.png)\n",
    "!['PCA Accuracy vs Number of Components'](images/lda_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first graph, we see significant improves over the standardization-only pipeline for the knn and svc_rbf models.  However, we see about equal of worse performance on all other models, inclusing a particularly large drop in accuracy for rand_for.  \n",
    "\n",
    "We may explore this preprocessing step in the future during during optimization.  We may also find it prudent to only apply this transformation to the models where it shows improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization\n",
    "Here we will perform hyperparameter tuning on each of the models and then investigate how we could possibly further improve the accuracy.\n",
    "### Random Forest\n",
    "[See Optimization- Random Forest Notebook](Optimization-RandomForest.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "### 6/28/2018\n",
    "#### Resampling before cross-validation\n",
    "Realized a mistake I had been making.  I was resampling all of the data before cross-validation.  This results in most of the data being present in both training and test data sets during cross-validation and thus overfitted models.\n",
    "\n",
    "I refactored the code to include the resampling (oversampling of the minority classes) as one of the steps in a pipeline that occurs during each cross-validation run.\n",
    "\n",
    "This has resulted in accuracies that are lower for cross-validation results, but are much more consistent between the cross-validation, holdout, and validation data (see updated accuracy charts in [Benchmark Results](#Results) and [Standardization](#Standardization) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
